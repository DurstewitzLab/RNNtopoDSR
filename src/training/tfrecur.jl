using Flux

using ..Models
using ..Utilities
using ..ObservationModels

abstract type AbstractTFRecur end
Flux.trainable(tfrec::AbstractTFRecur) = (tfrec.model, tfrec.O)

(tfrec::AbstractTFRecur)(X::AbstractArray{T, 3}) where {T} = forward(tfrec, X)

"""
    forward(tfrec, X)

Forward pass using teacher forcing. If the latent dimension of
the RNN is larger than the dimension the observations live in, 
partial teacher forcing of the first `N = size(X, 1)` neurons is
used. Initializing latent state `z‚ÇÅ` is taken care of by the observation model.
"""
function forward(tfrec::AbstractTFRecur, X::AbstractArray{T, 3}) where {T}
    N, _, TÃÉ = size(X)
    M = size(tfrec.z, 1)

    # number of forced states
    D = min(N, M)

    # precompute forcing signals
    Z‚É∞ = apply_inverse(tfrec.O, X)

    # initialize latent state
    tfrec.z = @views init_state(tfrec.O, X[:, :, 1])

    # process sequence X
    Z = @views [tfrec(Z‚É∞[1:D, :, t], t) for t = 2:TÃÉ]

    # reshape to 3d array and return
    return reshape(reduce(hcat, Z), size(tfrec.z)..., :)
end

"""
Inspired by `Flux.Recur` struct, which by default has no way
of incorporating teacher forcing.

This is just a convenience wrapper around stateful models,
to be used during training.
"""
mutable struct TFRecur{M <: AbstractMatrix, ùí™ <: ObservationModel} <: AbstractTFRecur
    # stateful model, e.g. PLRNN
    model::Any
    # observation model
    O::ùí™
    # state of the model
    z::M
    # forcing interval
    const œÑ::Int
end
Flux.@functor TFRecur

function (tfrec::TFRecur)(x::AbstractMatrix, t::Int)
    # determine if it is time to force the model
    z = tfrec.z

    # perform one step using the model, update model state
    z = tfrec.model(z)

    # force
    zÃÉ = (t - 1) % tfrec.œÑ == 0 ? force(z, x) : z
    tfrec.z = zÃÉ
    return z
end

